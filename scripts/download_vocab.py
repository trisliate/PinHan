"""
ä¸‹è½½å¹¶æ•´åˆç¬¬ä¸‰æ–¹è¯åº“

æ•°æ®æº:
1. mozillazg/phrase-pinyin-data - çŸ­è¯­æ‹¼éŸ³è¯åº“ (~9MB)
2. wainshine/Chinese-Names-Corpus - ä¸­æ–‡äººåè¯­æ–™åº“ (~12MB)

ç”¨æ³•:
    python scripts/download_vocab.py           # ä¸‹è½½ + æ•´åˆ
    python scripts/download_vocab.py --merge   # ä»…æ•´åˆ(å·²ä¸‹è½½)
    python scripts/download_vocab.py --download # ä»…ä¸‹è½½
"""
import argparse
import urllib.request
import orjson
from pathlib import Path
from pypinyin import lazy_pinyin

# ============ è·¯å¾„é…ç½® ============
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
SOURCES_DIR = PROJECT_ROOT / 'data' / 'sources'
DICTS_DIR = PROJECT_ROOT / 'data' / 'dicts'

# ============ ä¸‹è½½æº ============
VOCAB_SOURCES = {
    'phrase_pinyin.txt': {
        'url': 'https://raw.githubusercontent.com/mozillazg/phrase-pinyin-data/master/phrase_pinyin.txt',
        'desc': 'çŸ­è¯­æ‹¼éŸ³è¯åº“ (mozillazg/phrase-pinyin-data)',
    },
    'chinese_names.txt': {
        'url': 'https://raw.githubusercontent.com/wainshine/Chinese-Names-Corpus/master/Chinese_Names_Corpus/Chinese_Names_Corpus%EF%BC%88120W%EF%BC%89.txt',
        'desc': 'ä¸­æ–‡äººåè¯­æ–™åº“ (wainshine/Chinese-Names-Corpus)',
    },
}

# æ‹¼éŸ³å£°è°ƒè½¬æ¢è¡¨
TONE_MAP = {
    'Ä': 'a', 'Ã¡': 'a', 'Ç': 'a', 'Ã ': 'a',
    'Ä“': 'e', 'Ã©': 'e', 'Ä›': 'e', 'Ã¨': 'e',
    'Ä«': 'i', 'Ã­': 'i', 'Ç': 'i', 'Ã¬': 'i',
    'Å': 'o', 'Ã³': 'o', 'Ç’': 'o', 'Ã²': 'o',
    'Å«': 'u', 'Ãº': 'u', 'Ç”': 'u', 'Ã¹': 'u',
    'Ç–': 'v', 'Ç˜': 'v', 'Çš': 'v', 'Çœ': 'v', 'Ã¼': 'v',
    'Å„': 'n', 'Åˆ': 'n', 'Ç¹': 'n',
}


def remove_tone(pinyin: str) -> str:
    """ç§»é™¤æ‹¼éŸ³å£°è°ƒ"""
    return ''.join(TONE_MAP.get(c, c) for c in pinyin)


def download_file(url: str, dest: Path, desc: str) -> bool:
    """ä¸‹è½½æ–‡ä»¶"""
    print(f"ğŸ“¥ ä¸‹è½½ {desc}...")
    print(f"   URL: {url}")
    
    try:
        req = urllib.request.Request(
            url,
            headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        )
        
        with urllib.request.urlopen(req, timeout=60) as response:
            data = response.read()
            dest.write_bytes(data)
            size_mb = len(data) / 1024 / 1024
            print(f"   âœ“ æˆåŠŸ: {dest.name} ({size_mb:.1f} MB)")
            return True
            
    except Exception as e:
        print(f"   âœ— å¤±è´¥: {e}")
        return False


def download_all() -> bool:
    """ä¸‹è½½æ‰€æœ‰è¯åº“"""
    print("\n" + "=" * 50)
    print("ğŸ“¦ ä¸‹è½½ç¬¬ä¸‰æ–¹è¯åº“")
    print("=" * 50)
    
    SOURCES_DIR.mkdir(parents=True, exist_ok=True)
    
    success_count = 0
    for filename, info in VOCAB_SOURCES.items():
        dest = SOURCES_DIR / filename
        if dest.exists():
            size_mb = dest.stat().st_size / 1024 / 1024
            print(f"â­ï¸  è·³è¿‡ {filename} (å·²å­˜åœ¨, {size_mb:.1f} MB)")
            success_count += 1
        else:
            if download_file(info['url'], dest, info['desc']):
                success_count += 1
    
    print(f"\nä¸‹è½½å®Œæˆ: {success_count}/{len(VOCAB_SOURCES)}")
    return success_count == len(VOCAB_SOURCES)


def parse_phrase_pinyin(filepath: Path) -> dict[str, list[str]]:
    """è§£æ phrase_pinyin.txt"""
    pinyin_to_words = {}
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or ':' not in line:
                continue
            
            parts = line.split(':', 1)
            if len(parts) != 2:
                continue
            
            word = parts[0].strip()
            pinyin_with_tone = parts[1].strip()
            
            if len(word) < 2:
                continue
            
            syllables = pinyin_with_tone.split()
            pinyin = ''.join(remove_tone(s) for s in syllables)
            
            if pinyin not in pinyin_to_words:
                pinyin_to_words[pinyin] = []
            if word not in pinyin_to_words[pinyin]:
                pinyin_to_words[pinyin].append(word)
    
    return pinyin_to_words


def parse_chinese_names(filepath: Path) -> dict[str, list[str]]:
    """è§£æä¸­æ–‡äººåè¯­æ–™åº“"""
    pinyin_to_names = {}
    
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            name = line.strip()
            if not name or name.startswith(('By@', '#', '//', '2025')):
                continue
            
            if not (2 <= len(name) <= 4):
                continue
            if not all('\u4e00' <= c <= '\u9fff' for c in name):
                continue
            
            try:
                py_list = lazy_pinyin(name)
                pinyin = ''.join(py_list)
            except Exception:
                continue
            
            if pinyin not in pinyin_to_names:
                pinyin_to_names[pinyin] = []
            if name not in pinyin_to_names[pinyin]:
                pinyin_to_names[pinyin].append(name)
    
    return pinyin_to_names


def merge_vocab():
    """æ•´åˆè¯åº“åˆ°å­—å…¸"""
    print("\n" + "=" * 50)
    print("ğŸ”— æ•´åˆè¯åº“åˆ°å­—å…¸")
    print("=" * 50)
    
    word_dict_path = DICTS_DIR / 'word_dict.json'
    if word_dict_path.exists():
        word_dict = orjson.loads(word_dict_path.read_bytes())
        print(f"åŠ è½½ç°æœ‰è¯å…¸: {len(word_dict):,} æ¡")
    else:
        word_dict = {}
        print("âš ï¸  è¯å…¸ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ python scripts/build_dict.py")
        return False
    
    original_count = len(word_dict)
    new_pinyin_count = 0
    new_word_count = 0
    
    phrase_path = SOURCES_DIR / 'phrase_pinyin.txt'
    if phrase_path.exists():
        print(f"\nè§£æçŸ­è¯­è¯åº“...")
        phrase_vocab = parse_phrase_pinyin(phrase_path)
        print(f"  è·å– {len(phrase_vocab):,} ä¸ªæ‹¼éŸ³æ¡ç›®")
        
        for pinyin, words in phrase_vocab.items():
            if pinyin not in word_dict:
                word_dict[pinyin] = []
                new_pinyin_count += 1
            for word in words:
                if word not in word_dict[pinyin]:
                    word_dict[pinyin].append(word)
                    new_word_count += 1
    
    names_path = SOURCES_DIR / 'chinese_names.txt'
    if names_path.exists():
        print(f"\nè§£æäººåè¯­æ–™åº“...")
        names_vocab = parse_chinese_names(names_path)
        print(f"  è·å– {len(names_vocab):,} ä¸ªæ‹¼éŸ³æ¡ç›®")
        
        for pinyin, names in names_vocab.items():
            if pinyin not in word_dict:
                word_dict[pinyin] = []
                new_pinyin_count += 1
            for name in names:
                if name not in word_dict[pinyin]:
                    word_dict[pinyin].append(name)
                    new_word_count += 1
    
    word_dict_path.write_bytes(
        orjson.dumps(word_dict, option=orjson.OPT_INDENT_2)
    )
    
    print(f"\n" + "=" * 50)
    print(f"âœ… è¯å…¸æ›´æ–°å®Œæˆ")
    print(f"   åŸæ‹¼éŸ³æ¡ç›®: {original_count:,}")
    print(f"   æ–°å¢æ‹¼éŸ³: {new_pinyin_count:,}")
    print(f"   æ–°å¢è¯æ¡: {new_word_count:,}")
    print(f"   å½“å‰æ€»è®¡: {len(word_dict):,} æ‹¼éŸ³æ¡ç›®")
    
    print(f"\néªŒè¯å¸¸ç”¨äººå:")
    test_cases = ['xiaoming', 'xiaohua', 'zhangsan', 'lisi', 'wangwu', 'xiaohong']
    for py in test_cases:
        words = word_dict.get(py, [])
        if words:
            print(f"  âœ“ {py}: {words[:5]}{'...' if len(words) > 5 else ''}")
        else:
            print(f"  âœ— {py}: æœªæ‰¾åˆ°")
    
    return True


def main():
    parser = argparse.ArgumentParser(description="ä¸‹è½½å¹¶æ•´åˆç¬¬ä¸‰æ–¹è¯åº“")
    parser.add_argument('--download', action='store_true', help='ä»…ä¸‹è½½')
    parser.add_argument('--merge', action='store_true', help='ä»…æ•´åˆ')
    args = parser.parse_args()
    
    if args.download:
        download_all()
    elif args.merge:
        merge_vocab()
    else:
        if download_all():
            merge_vocab()


if __name__ == '__main__':
    main()
