#!/usr/bin/env python3
"""
å°è§„æ¨¡è®­ç»ƒè„šæœ¬ï¼šä»10k.jsonlä¸­éšæœºæŠ½å–500-3000è¡Œè¿›è¡Œå¿«é€Ÿè®­ç»ƒæµ‹è¯•ã€‚
ç”¨äºè¯„ä¼°æ¨¡å‹æ•ˆæœå’Œé¢„ä¼°ç”Ÿäº§è®­ç»ƒçš„è½®æ•°éœ€æ±‚ã€‚
"""
import random
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
import orjson

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)

sys.path.insert(0, str(Path(__file__).parent / 'model'))
sys.path.insert(0, str(Path(__file__).parent / 'preprocess'))

from seq2seq_transformer import Vocab, Seq2SeqTransformer, generate_square_subsequent_mask
from pinyin_utils import validate_pinyin_sequence
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim
import time


class SmallPinyinDataset(Dataset):
    """å°è§„æ¨¡æ‹¼éŸ³->æ±‰å­—æ•°æ®é›†."""
    def __init__(self, samples: list, src_vocab: Vocab, tgt_vocab: Vocab):
        self.samples = samples
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        return self.samples[idx]


def collate_fn(batch, src_vocab, tgt_vocab):
    """æ‰¹å¤„ç†å‡½æ•°."""
    srcs, tgts = zip(*batch)
    src_ids = [src_vocab.encode(list(s), add_bos_eos=True) for s in srcs]
    tgt_ids = [tgt_vocab.encode(list(t), add_bos_eos=True) for t in tgts]
    
    max_src = max(len(x) for x in src_ids)
    max_tgt = max(len(x) for x in tgt_ids)
    
    pad_id_src = src_vocab.token_to_id[src_vocab.pad_token]
    pad_id_tgt = tgt_vocab.token_to_id[tgt_vocab.pad_token]
    
    src_padded = [x + [pad_id_src] * (max_src - len(x)) for x in src_ids]
    tgt_padded = [x + [pad_id_tgt] * (max_tgt - len(x)) for x in tgt_ids]
    
    src_tensor = torch.LongTensor(src_padded).transpose(0, 1)
    tgt_tensor = torch.LongTensor(tgt_padded).transpose(0, 1)
    
    return src_tensor, tgt_tensor


def extract_small_dataset(input_file: Path, sample_size: int = 1000) -> list:
    """
    ä»10k.jsonlä¸­éšæœºæŠ½å–æŒ‡å®šæ•°é‡çš„æ ·æœ¬ã€‚
    è¿”å› [(src_tokens, tgt_tokens), ...] åˆ—è¡¨
    """
    logger.info(f"ä» {input_file} è¯»å–æ‰€æœ‰æ•°æ®...")
    all_samples = []
    
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                j = orjson.loads(line)
            except:
                continue
            
            pinyin = j.get('pinyin', '').strip()
            hanzi = j.get('hanzi', '').strip()
            
            if not pinyin or not hanzi:
                continue
            
            # éªŒè¯æ‹¼éŸ³
            if not validate_pinyin_sequence(pinyin):
                continue
            
            src_tokens = pinyin.split()
            tgt_tokens = list(hanzi)
            
            # é•¿åº¦åŒ¹é…æ£€æŸ¥
            if len(src_tokens) != len(tgt_tokens):
                continue
            
            all_samples.append((src_tokens, tgt_tokens))
    
    logger.info(f"æ€»å…±æ‰¾åˆ° {len(all_samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    
    # éšæœºæŠ½å–
    if len(all_samples) > sample_size:
        selected = random.sample(all_samples, sample_size)
    else:
        selected = all_samples
    
    logger.info(f"æŠ½å–äº† {len(selected)} ä¸ªæ ·æœ¬ç”¨äºè®­ç»ƒ")
    return selected


def build_vocabs_from_samples(samples: list) -> tuple:
    """ä»æ ·æœ¬ä¸­æ„å»ºè¯è¡¨."""
    pinyin_set = set()
    hanzi_set = set()
    
    for src_tokens, tgt_tokens in samples:
        pinyin_set.update(src_tokens)
        hanzi_set.update(tgt_tokens)
    
    src_vocab = Vocab(list(pinyin_set))
    tgt_vocab = Vocab(list(hanzi_set))
    
    logger.info(f"æºè¯è¡¨å¤§å°: {len(src_vocab)}")
    logger.info(f"ç›®æ ‡è¯è¡¨å¤§å°: {len(tgt_vocab)}")
    
    return src_vocab, tgt_vocab


def train_small_model(
    samples: list,
    epochs: int = 50,
    batch_size: int = 4,
    learning_rate: float = 1e-4,
    output_dir: Path = None
) -> dict:
    """
    å°è§„æ¨¡æ¨¡å‹è®­ç»ƒã€‚
    è¿”å› {epoch -> {loss, metrics}}
    """
    if output_dir is None:
        output_dir = Path('outputs/small_train')
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {DEVICE}")
    
    # æ„å»ºè¯è¡¨
    src_vocab, tgt_vocab = build_vocabs_from_samples(samples)
    
    # åˆ›å»ºæ•°æ®é›†å’Œdataloader
    dataset = SmallPinyinDataset(samples, src_vocab, tgt_vocab)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab),
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = Seq2SeqTransformer(
        len(src_vocab),
        len(tgt_vocab),
        d_model=256,
        nhead=4,
        num_encoder_layers=3,
        num_decoder_layers=3,
        pad_idx_src=src_vocab.token_to_id[src_vocab.pad_token],
        pad_idx_tgt=tgt_vocab.token_to_id[tgt_vocab.pad_token],
    )
    model = model.to(DEVICE)
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.CrossEntropyLoss(
        ignore_index=tgt_vocab.token_to_id[tgt_vocab.pad_token]
    )
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2
    )
    
    logger.info(f"å¼€å§‹è®­ç»ƒ {epochs} è½®ï¼Œæ ·æœ¬æ•°: {len(samples)}")
    logger.info(f"æ‰¹å¤§å°: {batch_size}ï¼Œå­¦ä¹ ç‡: {learning_rate}")
    
    history = {}
    train_start = time.time()
    
    for epoch in range(1, epochs + 1):
        epoch_start = time.time()
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        for batch_idx, (src, tgt) in enumerate(dataloader):
            src = src.to(DEVICE)
            tgt = tgt.to(DEVICE)
            
            tgt_input = tgt[:-1, :]
            tgt_out = tgt[1:, :]
            
            tgt_mask = generate_square_subsequent_mask(tgt_input.size(0)).to(DEVICE)
            src_key_padding_mask = (
                src.transpose(0, 1) == src_vocab.token_to_id[src_vocab.pad_token]
            )
            tgt_key_padding_mask = (
                tgt_input.transpose(0, 1) == tgt_vocab.token_to_id[tgt_vocab.pad_token]
            )
            
            # è½¬æ¢maskä¸ºfloat
            tgt_mask = tgt_mask.float()
            src_key_padding_mask = src_key_padding_mask.float()
            tgt_key_padding_mask = tgt_key_padding_mask.float()
            
            optimizer.zero_grad()
            
            logits = model(
                src,
                tgt_input,
                tgt_mask=tgt_mask,
                src_key_padding_mask=src_key_padding_mask,
                tgt_key_padding_mask=tgt_key_padding_mask,
                memory_key_padding_mask=src_key_padding_mask,
            )
            
            loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.reshape(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
            
            if batch_idx % max(1, len(dataloader) // 5) == 0:
                logger.info(
                    f"Epoch {epoch}/{epochs} [{batch_idx+1}/{len(dataloader)}] "
                    f"loss={loss.item():.4f}"
                )
        
        avg_loss = total_loss / num_batches
        scheduler.step(avg_loss)
        epoch_time = time.time() - epoch_start
        
        history[epoch] = {
            'loss': avg_loss,
            'time': epoch_time,
            'lr': optimizer.param_groups[0]['lr']
        }
        
        logger.info(
            f"Epoch {epoch} å®Œæˆ | Loss: {avg_loss:.4f} | "
            f"Time: {epoch_time:.1f}s | LR: {optimizer.param_groups[0]['lr']:.6f}"
        )
    
    total_time = time.time() - train_start
    logger.info(f"âœ… è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}s")
    
    # ä¿å­˜æ¨¡å‹å’Œè¯è¡¨
    torch.save(model.state_dict(), output_dir / 'model.pt')
    src_vocab.save(str(output_dir / 'src_vocab.json'))
    tgt_vocab.save(str(output_dir / 'tgt_vocab.json'))
    
    logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")
    
    return history


def analyze_training_curve(history: dict) -> dict:
    """
    åˆ†æè®­ç»ƒæ›²çº¿ï¼Œé¢„ä¼°è¾¾åˆ°ç›®æ ‡ç²¾åº¦éœ€è¦çš„è½®æ•°ã€‚
    
    è¿”å› {
        'initial_loss': float,
        'final_loss': float,
        'loss_reduction_percent': float,
        'avg_loss_per_epoch': float,
        'estimated_epochs_for_99_percent': int,
    }
    """
    losses = [h['loss'] for h in history.values()]
    
    initial_loss = losses[0]
    final_loss = losses[-1]
    reduction_percent = (initial_loss - final_loss) / initial_loss * 100
    avg_loss_per_epoch = sum(losses) / len(losses)
    
    # ç®€å•çš„çº¿æ€§å¤–æ¨
    if final_loss < initial_loss:
        loss_per_epoch = (initial_loss - final_loss) / len(losses)
        # ç›®æ ‡ï¼šæŸå¤±é™ä½åˆ°åˆå§‹æŸå¤±çš„5%
        target_loss = initial_loss * 0.05
        remaining_epochs = max(0, int((final_loss - target_loss) / loss_per_epoch))
        estimated_epochs = len(losses) + remaining_epochs
    else:
        estimated_epochs = len(losses) * 2  # ä¿å®ˆä¼°è®¡ç¿»å€
    
    return {
        'initial_loss': initial_loss,
        'final_loss': final_loss,
        'loss_reduction_percent': reduction_percent,
        'avg_loss_per_epoch': avg_loss_per_epoch,
        'estimated_epochs_for_convergence': estimated_epochs,
        'total_epochs_trained': len(losses),
    }


def main():
    """ä¸»å‡½æ•°."""
    import argparse
    
    parser = argparse.ArgumentParser(description='å°è§„æ¨¡è®­ç»ƒè„šæœ¬')
    parser.add_argument('--input', type=str, default='data/10k.jsonl', help='è¾“å…¥æ•°æ®æ–‡ä»¶')
    parser.add_argument('--sample-size', type=int, default=1000, help='æŠ½å–æ ·æœ¬æ•° (500-3000)')
    parser.add_argument('--epochs', type=int, default=50, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=4, help='æ‰¹å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--output-dir', type=str, default='outputs/small_train', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    input_file = Path(args.input)
    if not input_file.exists():
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        sys.exit(1)
    
    # ç¡®ä¿æ ·æœ¬å¤§å°åœ¨åˆç†èŒƒå›´
    sample_size = max(500, min(3000, args.sample_size))
    logger.info(f"ç›®æ ‡æ ·æœ¬æ•°: {sample_size}")
    
    # æå–æ•°æ®
    samples = extract_small_dataset(input_file, sample_size)
    
    if len(samples) < 10:
        logger.error("æ ·æœ¬å¤ªå°‘ï¼Œæ— æ³•è®­ç»ƒ")
        sys.exit(1)
    
    # è®­ç»ƒæ¨¡å‹
    history = train_small_model(
        samples,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        output_dir=Path(args.output_dir)
    )
    
    # åˆ†æè®­ç»ƒæ›²çº¿
    analysis = analyze_training_curve(history)
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š è®­ç»ƒåˆ†æç»“æœ")
    logger.info("="*60)
    logger.info(f"åˆå§‹æŸå¤±: {analysis['initial_loss']:.4f}")
    logger.info(f"æœ€ç»ˆæŸå¤±: {analysis['final_loss']:.4f}")
    logger.info(f"æŸå¤±ä¸‹é™: {analysis['loss_reduction_percent']:.2f}%")
    logger.info(f"å¹³å‡æ¯è½®æŸå¤±: {analysis['avg_loss_per_epoch']:.4f}")
    logger.info(f"å®é™…è®­ç»ƒè½®æ•°: {analysis['total_epochs_trained']}")
    logger.info(f"é¢„ä¼°æ”¶æ•›è½®æ•°: {analysis['estimated_epochs_for_convergence']}")
    logger.info("="*60)
    
    # ä¿å­˜åˆ†æç»“æœ
    output_dir = Path(args.output_dir)
    with open(output_dir / 'training_analysis.json', 'w', encoding='utf-8') as f:
        json.dump({
            'analysis': analysis,
            'history': history,
            'timestamp': datetime.now().isoformat(),
            'config': {
                'sample_size': len(samples),
                'epochs': args.epochs,
                'batch_size': args.batch_size,
                'learning_rate': args.learning_rate,
            }
        }, f, indent=2, ensure_ascii=False)
    
    logger.info(f"\nè¯¦ç»†åˆ†æå·²ä¿å­˜åˆ° {output_dir}/training_analysis.json")


if __name__ == '__main__':
    main()
