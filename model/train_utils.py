"""
æ”¹è¿›ç‰ˆè®­ç»ƒè„šæœ¬ - æ”¯æŒå®Œæ•´çš„å¢é‡è®­ç»ƒå’Œæ¨¡å‹ç‰ˆæœ¬ç®¡ç†

å…³é”®æ”¹è¿›:
âœ… æ¨¡å‹ä¿å­˜æ—¶åŒ…å«å®Œæ•´å…ƒæ•°æ®
âœ… æ”¯æŒè¯è¡¨ç‰ˆæœ¬æ£€æŸ¥
âœ… å¢é‡è®­ç»ƒå‰æ£€æŸ¥å…¼å®¹æ€§
âœ… è‡ªåŠ¨è¯è¡¨æ‰©å±•
âœ… è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—
"""
import sys
import logging
from pathlib import Path
from datetime import datetime
from typing import Tuple, Dict, Any
import argparse
import orjson
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim

sys.path.insert(0, str(Path(__file__).parent.parent / 'preprocess'))
from seq2seq_transformer import Vocab, Seq2SeqTransformer, generate_square_subsequent_mask
from pinyin_utils import normalize_pinyin_sequence, validate_pinyin_sequence


def get_device():
    """æ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆ NVIDIA CUDAï¼Œå¦åˆ™ CPU"""
    if torch.cuda.is_available():
        device = torch.device('cuda')
        device_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        logging.info(f"âœ… ä½¿ç”¨ NVIDIA GPU: {device_name} ({memory_gb:.2f}GB)")
        return device
    
    # CPU è®­ç»ƒ
    num_cores = torch.get_num_threads()
    logging.info(f"âœ… ä½¿ç”¨ CPU è®­ç»ƒ ({num_cores}æ ¸)")
    torch.set_num_threads(num_cores)
    return torch.device('cpu')

DEVICE = get_device()


def get_model_version() -> str:
    """è·å–æ¨¡å‹ç‰ˆæœ¬å· (åŸºäºä»£ç æäº¤IDæˆ–æ—¶é—´æˆ³)."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def create_model_metadata(
    model: nn.Module,
    epoch: int,
    loss: float,
    accuracy: float,
    data_source: str,
    src_vocab: Vocab,
    tgt_vocab: Vocab,
    config: Dict[str, Any],
    training_config: Dict[str, Any],
) -> Dict[str, Any]:
    """åˆ›å»ºæ¨¡å‹å…ƒæ•°æ®."""
    return {
        # ç‰ˆæœ¬ä¿¡æ¯
        'version': '2.0.0',  # å¢é‡è®­ç»ƒæ”¯æŒç‰ˆæœ¬
        'timestamp': datetime.now().isoformat(),
        'device': str(DEVICE),
        
        # è®­ç»ƒä¿¡æ¯
        'metadata': {
            'epoch': epoch,
            'loss': float(loss),
            'accuracy': float(accuracy),
            'data_source': str(data_source),
            'total_samples': None,  # ç”±è°ƒç”¨è€…å¡«å……
        },
        
        # æ¨¡å‹æ¶æ„é…ç½®
        'config': config,
        
        # è¯è¡¨ä¿¡æ¯
        'vocab_info': {
            'src_vocab_size': len(src_vocab),
            'tgt_vocab_size': len(tgt_vocab),
            'src_special_tokens': {
                'pad': src_vocab.pad_token,
                'unk': src_vocab.unk_token,
            },
            'tgt_special_tokens': {
                'pad': tgt_vocab.pad_token,
                'unk': tgt_vocab.unk_token,
            },
            'src_vocab_hash': hash_vocab(src_vocab),
            'tgt_vocab_hash': hash_vocab(tgt_vocab),
        },
        
        # è®­ç»ƒè¶…å‚æ•°
        'training_config': training_config,
        
        # æ•°æ®é›†ä¿¡æ¯
        'dataset_info': {
            'max_src_len': 64,
            'max_tgt_len': 64,
        }
    }


def hash_vocab(vocab: Vocab) -> str:
    """è®¡ç®—è¯è¡¨çš„å“ˆå¸Œå€¼ï¼Œç”¨äºç‰ˆæœ¬æ¯”å¯¹."""
    import hashlib
    tokens = sorted(vocab.token_to_id.keys())
    content = orjson.dumps(tokens).decode('utf-8')
    return hashlib.md5(content.encode()).hexdigest()


def check_vocab_compatibility(
    saved_metadata: Dict[str, Any],
    current_src_vocab: Vocab,
    current_tgt_vocab: Vocab,
    strategy: str = 'expand',
) -> Tuple[bool, str]:
    """
    æ£€æŸ¥è¯è¡¨å…¼å®¹æ€§ã€‚
    
    Args:
        saved_metadata: ä¿å­˜çš„æ¨¡å‹å…ƒæ•°æ®
        current_src_vocab: å½“å‰æºè¯è¡¨
        current_tgt_vocab: å½“å‰ç›®æ ‡è¯è¡¨
        strategy: 'strict'(ä¸¥æ ¼) or 'expand'(æ‰©å±•)
    
    Returns:
        (compatible, message)
    """
    saved_src_hash = saved_metadata['vocab_info'].get('src_vocab_hash')
    saved_tgt_hash = saved_metadata['vocab_info'].get('tgt_vocab_hash')
    
    current_src_hash = hash_vocab(current_src_vocab)
    current_tgt_hash = hash_vocab(current_tgt_vocab)
    
    src_match = saved_src_hash == current_src_hash
    tgt_match = saved_tgt_hash == current_tgt_hash
    
    if strategy == 'strict':
        # ä¸¥æ ¼æ¨¡å¼: è¯è¡¨å¿…é¡»å®Œå…¨ç›¸åŒ
        if not (src_match and tgt_match):
            msg = f"è¯è¡¨ä¸å…¼å®¹ (strict mode):\n"
            msg += f"  æºè¯è¡¨: {'âœ“' if src_match else 'âœ—'}\n"
            msg += f"  ç›®æ ‡è¯è¡¨: {'âœ“' if tgt_match else 'âœ—'}\n"
            return False, msg
        return True, "è¯è¡¨å®Œå…¨åŒ¹é… âœ“"
    
    elif strategy == 'expand':
        # æ‰©å±•æ¨¡å¼: å…è®¸è¯è¡¨å¢é•¿ (å‘åå…¼å®¹)
        src_size_ok = len(current_src_vocab) >= saved_metadata['vocab_info']['src_vocab_size']
        tgt_size_ok = len(current_tgt_vocab) >= saved_metadata['vocab_info']['tgt_vocab_size']
        
        if not (src_size_ok and tgt_size_ok):
            msg = f"è¯è¡¨ç¼©å°äº†ï¼Œä¸å…è®¸ (expand mode):\n"
            msg += f"  æºè¯è¡¨: {saved_metadata['vocab_info']['src_vocab_size']} â†’ {len(current_src_vocab)}\n"
            msg += f"  ç›®æ ‡è¯è¡¨: {saved_metadata['vocab_info']['tgt_vocab_size']} â†’ {len(current_tgt_vocab)}\n"
            return False, msg
        
        if src_match and tgt_match:
            return True, "è¯è¡¨å®Œå…¨åŒ¹é… âœ“"
        else:
            msg = f"è¯è¡¨å·²æ‰©å±•ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ:\n"
            msg += f"  æºè¯è¡¨: {saved_metadata['vocab_info']['src_vocab_size']} â†’ {len(current_src_vocab)}\n"
            msg += f"  ç›®æ ‡è¯è¡¨: {saved_metadata['vocab_info']['tgt_vocab_size']} â†’ {len(current_tgt_vocab)}\n"
            return True, msg
    
    return False, "æœªçŸ¥çš„æ£€æŸ¥ç­–ç•¥"


def save_model_complete(
    model: nn.Module,
    optimizer: optim.Optimizer,
    epoch: int,
    loss: float,
    accuracy: float,
    data_source: str,
    src_vocab: Vocab,
    tgt_vocab: Vocab,
    config: Dict[str, Any],
    training_config: Dict[str, Any],
    out_dir: Path,
) -> None:
    """
    å®Œæ•´åœ°ä¿å­˜æ¨¡å‹ã€‚
    
    ä¿å­˜ç»“æ„:
    outputs/
      â”œâ”€â”€ model.pt          (æœ€æ–°å®Œæ•´æ¨¡å‹)
      â”œâ”€â”€ metadata.json     (å…ƒæ•°æ®)
      â”œâ”€â”€ src_vocab.json    (æºè¯è¡¨)
      â”œâ”€â”€ tgt_vocab.json    (ç›®æ ‡è¯è¡¨)
      â”œâ”€â”€ checkpoint_epoch1.pt
      â”œâ”€â”€ checkpoint_epoch2.pt
      â””â”€â”€ ...
    """
    out_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºå…ƒæ•°æ®
    metadata = create_model_metadata(
        model, epoch, loss, accuracy, data_source,
        src_vocab, tgt_vocab, config, training_config
    )
    
    # ä¿å­˜checkpoint (ç”¨äºresume)
    ckpt = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'metadata': metadata,
    }
    torch.save(ckpt, out_dir / f'checkpoint_epoch{epoch}.pt')
    
    # ä¿å­˜æœ€æ–°å®Œæ•´æ¨¡å‹ (ç”¨äºæ¨ç†)
    final_model = {
        'model_state_dict': model.state_dict(),
        'metadata': metadata,
        'config': config,
    }
    torch.save(final_model, out_dir / 'model.pt')
    
    # ä¿å­˜å…ƒæ•°æ®ä¸ºJSON (ä¾¿äºæŸ¥çœ‹)
    with open(out_dir / 'metadata.json', 'wb') as f:
        f.write(orjson.dumps(metadata, option=orjson.OPT_INDENT_2))
    
    # ä¿å­˜è¯è¡¨
    src_vocab.save(str(out_dir / 'src_vocab.json'))
    tgt_vocab.save(str(out_dir / 'tgt_vocab.json'))
    
    logger = logging.getLogger()
    logger.info(f"âœ… å®Œæ•´æ¨¡å‹å·²ä¿å­˜åˆ° {out_dir}")
    logger.info(f"   - model.pt (æ¨ç†ç”¨)")
    logger.info(f"   - checkpoint_epoch{epoch}.pt (æ¢å¤è®­ç»ƒç”¨)")
    logger.info(f"   - metadata.json (å…ƒæ•°æ®)")
    logger.info(f"   - src/tgt_vocab.json (è¯è¡¨)")


def load_model_for_resume(
    model_dir: Path,
    device: torch.device,
    strategy: str = 'expand',
) -> Tuple[Dict[str, Any], Dict[str, Any], int]:
    """
    åŠ è½½æ¨¡å‹ç”¨äºç»§ç»­è®­ç»ƒã€‚
    
    Returns:
        (model_state, metadata, start_epoch)
    """
    logger = logging.getLogger()
    
    # æŸ¥æ‰¾æœ€æ–°çš„checkpoint
    ckpts = sorted(model_dir.glob('checkpoint_epoch*.pt'))
    if not ckpts:
        logger.warning(f"æœªæ‰¾åˆ°checkpointï¼Œæ— æ³•æ¢å¤")
        return None, None, 1
    
    latest_ckpt = ckpts[-1]
    logger.info(f"ğŸ“‚ åŠ è½½checkpoint: {latest_ckpt.name}")
    
    # åŠ è½½checkpoint
    ck = torch.load(str(latest_ckpt), map_location=device)
    model_state = ck['model_state_dict']
    metadata = ck.get('metadata', {})
    start_epoch = ck.get('epoch', 1) + 1
    
    logger.info(f"   - ä» Epoch {ck.get('epoch', '?')} æ¢å¤")
    logger.info(f"   - Loss: {metadata.get('metadata', {}).get('loss', '?'):.4f}")
    logger.info(f"   - ä¸‹ä¸€è½®ä» Epoch {start_epoch} å¼€å§‹")
    
    return model_state, metadata, start_epoch


def compare_models(model_dir1: Path, model_dir2: Path) -> None:
    """
    å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„å…ƒæ•°æ®ã€‚
    
    ç”¨é€”: å¯¹æ¯”ä¸åŒç‰ˆæœ¬çš„æ¨¡å‹æ€§èƒ½
    """
    logger = logging.getLogger()
    
    def load_meta(path):
        with open(path / 'metadata.json', 'rb') as f:
            return orjson.loads(f.read())
    
    meta1 = load_meta(model_dir1)
    meta2 = load_meta(model_dir2)
    
    logger.info("\n" + "="*60)
    logger.info("ğŸ” æ¨¡å‹å¯¹æ¯”")
    logger.info("="*60)
    
    logger.info(f"\nã€æ¨¡å‹1ã€‘{model_dir1.name}")
    logger.info(f"  ç‰ˆæœ¬: {meta1.get('version')}")
    logger.info(f"  Epoch: {meta1['metadata']['epoch']}")
    logger.info(f"  Loss: {meta1['metadata']['loss']:.4f}")
    logger.info(f"  å‡†ç¡®åº¦: {meta1['metadata']['accuracy']:.4f}")
    
    logger.info(f"\nã€æ¨¡å‹2ã€‘{model_dir2.name}")
    logger.info(f"  ç‰ˆæœ¬: {meta2.get('version')}")
    logger.info(f"  Epoch: {meta2['metadata']['epoch']}")
    logger.info(f"  Loss: {meta2['metadata']['loss']:.4f}")
    logger.info(f"  å‡†ç¡®åº¦: {meta2['metadata']['accuracy']:.4f}")
    
    logger.info("\n")


# ============================================================================
# ä»¥ä¸‹æ˜¯è°ƒç”¨ç¤ºä¾‹ (å®é™…åœ¨ train_pinhan.py ä¸­ä½¿ç”¨)
# ============================================================================

def example_training():
    """ç¤ºä¾‹: å¦‚ä½•ä½¿ç”¨æ–°çš„ä¿å­˜æœºåˆ¶."""
    
    # 1. è®­ç»ƒé…ç½®
    config = {
        'd_model': 256,
        'nhead': 4,
        'num_encoder_layers': 3,
        'num_decoder_layers': 3,
        'max_seq_len': 64,
    }
    
    training_config = {
        'batch_size': 4,
        'learning_rate': 0.0001,
        'optimizer': 'Adam',
        'scheduler': 'ReduceLROnPlateau',
    }
    
    # 2. åˆå§‹åŒ–æ¨¡å‹ (ä¼ªä»£ç )
    # model = ...
    # optimizer = ...
    # src_vocab, tgt_vocab = ...
    
    # 3. è®­ç»ƒ...
    # for epoch in range(epochs):
    #     loss = train_one_epoch(...)
    #     accuracy = evaluate(...)
    
    # 4. ä¿å­˜æ¨¡å‹ (æ”¹è¿›çš„æ–¹æ³•)
    # save_model_complete(
    #     model, optimizer, epoch, loss, accuracy,
    #     data_source='data/10k.jsonl',
    #     src_vocab=src_vocab,
    #     tgt_vocab=tgt_vocab,
    #     config=config,
    #     training_config=training_config,
    #     out_dir=Path('outputs/final_model')
    # )
    
    # 5. å¢é‡è®­ç»ƒ (åç»­)
    # å½“æœ‰æ–°æ•°æ®æ—¶:
    # - åŠ è½½æ—§æ¨¡å‹å…ƒæ•°æ®
    # - é‡æ–°æ„å»ºè¯è¡¨ (å¯èƒ½åŒ…å«æ–°è¯)
    # - æ£€æŸ¥å…¼å®¹æ€§
    # - ç»§ç»­è®­ç»ƒ
    
    print("å‚è€ƒ save_model_complete() å’Œ load_model_for_resume()")


if __name__ == '__main__':
    example_training()
