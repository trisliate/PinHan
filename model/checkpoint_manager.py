"""
å®Œæ•´çš„è®­ç»ƒä¿å­˜å’Œæ–­ç‚¹ç®¡ç†ç³»ç»Ÿ - æ”¯æŒå¢é‡è®­ç»ƒå’Œå®Œæ•´æ¢å¤
"""
import logging
import orjson
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
import torch
import torch.nn as nn


class TrainingCheckpointManager:
    """
    å®Œæ•´çš„è®­ç»ƒç®¡ç†ç³»ç»Ÿï¼š
    - è‡ªåŠ¨ä¿å­˜æœ€ä¼˜æ¨¡å‹
    - æ™ºèƒ½ç®¡ç†æ£€æŸ¥ç‚¹ï¼ˆåªä¿ç•™å¿…è¦çš„ï¼‰
    - å®Œæ•´çš„å¢é‡è®­ç»ƒæ¢å¤
    - æ–­ç‚¹ç»­è®­æ”¯æŒ
    """
    
    def __init__(self, save_dir: Path, keep_checkpoints: int = 3):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        self.keep_checkpoints = keep_checkpoints
        
        self.logger = logging.getLogger(__name__)
        self.best_loss = float('inf')
        self.best_epoch = 0
        self.training_history = []
        
        # æ—¥å¿—å’Œé…ç½®ç›®å½•
        self.log_dir = self.save_dir / 'logs'
        self.log_dir.mkdir(exist_ok=True)
    
    def save_checkpoint(
        self,
        epoch: int,
        model: nn.Module,
        optimizer,
        loss: float,
        src_vocab,
        tgt_vocab,
        metrics: Dict[str, float] = None,
    ) -> Path:
        """
        ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆåŒ…å«å®Œæ•´çš„æ¢å¤ä¿¡æ¯ï¼‰ã€‚
        
        Args:
            epoch: epoch å·
            model: æ¨¡å‹
            optimizer: ä¼˜åŒ–å™¨
            loss: å½“å‰ loss
            src_vocab: æºè¯è¡¨
            tgt_vocab: ç›®æ ‡è¯è¡¨
            metrics: é¢å¤–æŒ‡æ ‡
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        checkpoint = {
            # åŸºæœ¬ä¿¡æ¯
            'epoch': epoch,
            'loss': float(loss),
            'timestamp': datetime.now().isoformat(),
            
            # æ¨¡å‹å’Œä¼˜åŒ–å™¨
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            
            # è¯è¡¨éªŒè¯ä¿¡æ¯
            'src_vocab_size': len(src_vocab),
            'tgt_vocab_size': len(tgt_vocab),
            'src_vocab_tokens': len(src_vocab.id_to_token),
            'tgt_vocab_tokens': len(tgt_vocab.id_to_token),
            
            # æ¢å¤ä¿¡æ¯
            'recovery_info': {
                'can_resume': True,
                'next_epoch': epoch + 1,
                'best_loss_so_far': self.best_loss,
                'best_epoch_so_far': self.best_epoch,
            },
            
            # é¢å¤–æŒ‡æ ‡
            'metrics': metrics or {},
        }
        
        ckpt_path = self.save_dir / f'checkpoint_epoch{epoch}.pt'
        torch.save(checkpoint, ckpt_path)
        self.logger.debug(f"âœ… ä¿å­˜æ£€æŸ¥ç‚¹: {ckpt_path.name}")
        
        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self._cleanup_old_checkpoints()
        
        return ckpt_path
    
    def save_best_model(
        self,
        epoch: int,
        model: nn.Module,
        optimizer,
        loss: float,
        src_vocab,
        tgt_vocab,
        metrics: Dict[str, float] = None,
    ) -> Tuple[Path, bool]:
        """
        ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆå¦‚æœå½“å‰lossæ›´ä½ï¼‰ã€‚
        
        Returns:
            (ä¿å­˜è·¯å¾„, æ˜¯å¦ä¸ºæ–°çš„æœ€ä¼˜æ¨¡å‹)
        """
        is_new_best = loss < self.best_loss
        
        if is_new_best:
            self.best_loss = loss
            self.best_epoch = epoch
            
            best_model = {
                # åŸºæœ¬ä¿¡æ¯
                'epoch': epoch,
                'loss': float(loss),
                'timestamp': datetime.now().isoformat(),
                
                # æ¨¡å‹
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                
                # è¯è¡¨
                'src_vocab_size': len(src_vocab),
                'tgt_vocab_size': len(tgt_vocab),
                'src_vocab_tokens': len(src_vocab.id_to_token),
                'tgt_vocab_tokens': len(tgt_vocab.id_to_token),
                
                # å…ƒæ•°æ®
                'best_flag': True,
                'metrics': metrics or {},
            }
            
            best_path = self.save_dir / 'best_model.pt'
            torch.save(best_model, best_path)
            self.logger.info(f"ğŸŒŸ æ–°çš„æœ€ä¼˜æ¨¡å‹: Loss={loss:.4f} at Epoch {epoch}")
            
            return best_path, True
        
        return None, False
    
    def _cleanup_old_checkpoints(self):
        """åªä¿ç•™æœ€è¿‘ K ä¸ªæ£€æŸ¥ç‚¹ï¼Œåˆ é™¤æ—§çš„ã€‚"""
        ckpts = sorted(self.save_dir.glob('checkpoint_epoch*.pt'))
        
        if len(ckpts) > self.keep_checkpoints:
            to_delete = ckpts[:-self.keep_checkpoints]
            for ckpt in to_delete:
                ckpt.unlink()
                self.logger.debug(f"ğŸ—‘ï¸  åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {ckpt.name}")
    
    def try_resume_from_checkpoint(self, model, optimizer, device='cpu'):
        """
        å°è¯•ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚
        
        Returns:
            (æ˜¯å¦æ¢å¤æˆåŠŸ, ä¸‹ä¸€ä¸ªepoch, æ£€æŸ¥ç‚¹ä¿¡æ¯)
        """
        ckpts = sorted(self.save_dir.glob('checkpoint_epoch*.pt'))
        if not ckpts:
            self.logger.info("â„¹ï¸  æ²¡æœ‰æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            return False, 1, None
        
        latest_ckpt = ckpts[-1]
        self.logger.info(f"ğŸ“‚ æ‰¾åˆ°æ£€æŸ¥ç‚¹: {latest_ckpt.name}")
        
        try:
            ckpt = torch.load(str(latest_ckpt), map_location=device)
            
            # éªŒè¯æ£€æŸ¥ç‚¹å®Œæ•´æ€§
            required_keys = ['model_state_dict', 'optimizer_state_dict', 'epoch', 'loss']
            missing = [k for k in required_keys if k not in ckpt]
            if missing:
                self.logger.error(f"âŒ æ£€æŸ¥ç‚¹ç¼ºå°‘å¿…è¦å­—æ®µ: {missing}")
                return False, 1, None
            
            # æ¢å¤æ¨¡å‹å’Œä¼˜åŒ–å™¨
            model.load_state_dict(ckpt['model_state_dict'])
            optimizer.load_state_dict(ckpt['optimizer_state_dict'])
            
            # æ¢å¤æœ€ä¼˜å€¼
            if 'recovery_info' in ckpt:
                recovery = ckpt['recovery_info']
                self.best_loss = recovery.get('best_loss_so_far', float('inf'))
                self.best_epoch = recovery.get('best_epoch_so_far', 0)
            
            next_epoch = ckpt.get('recovery_info', {}).get('next_epoch', ckpt['epoch'] + 1)
            
            self.logger.info(
                f"âœ… å·²æ¢å¤: Epoch {ckpt['epoch']}, Loss={ckpt['loss']:.4f}\n"
                f"   ä¸‹ä¸€è½®ä» Epoch {next_epoch} å¼€å§‹\n"
                f"   å½“å‰æœ€ä¼˜: Loss={self.best_loss:.4f} at Epoch {self.best_epoch}"
            )
            
            return True, next_epoch, ckpt
            
        except Exception as e:
            self.logger.error(f"âŒ æ¢å¤å¤±è´¥: {e}")
            return False, 1, None
    
    def validate_vocab_compatibility(
        self,
        ckpt: Dict,
        src_vocab_size: int,
        tgt_vocab_size: int,
        allow_size_increase: bool = False,
    ) -> Tuple[bool, str]:
        """
        éªŒè¯æ£€æŸ¥ç‚¹çš„è¯è¡¨ä¸å½“å‰è¯è¡¨æ˜¯å¦å…¼å®¹ã€‚
        
        Args:
            ckpt: æ£€æŸ¥ç‚¹æ•°æ®
            src_vocab_size: å½“å‰æºè¯è¡¨å¤§å°
            tgt_vocab_size: å½“å‰ç›®æ ‡è¯è¡¨å¤§å°
            allow_size_increase: æ˜¯å¦å…è®¸è¯è¡¨æ‰©å±•ï¼ˆç”¨äºå¢é‡è®­ç»ƒï¼‰
        
        Returns:
            (æ˜¯å¦å…¼å®¹, æ¶ˆæ¯)
        """
        ckpt_src = ckpt.get('src_vocab_size')
        ckpt_tgt = ckpt.get('tgt_vocab_size')
        
        if ckpt_src is None or ckpt_tgt is None:
            return True, "âœ… æ—§æ£€æŸ¥ç‚¹ï¼Œè·³è¿‡è¯è¡¨éªŒè¯"
        
        messages = []
        
        # æºè¯è¡¨æ£€æŸ¥
        if ckpt_src != src_vocab_size:
            if allow_size_increase and src_vocab_size > ckpt_src:
                messages.append(
                    f"â„¹ï¸  æºè¯è¡¨æ‰©å±•: {ckpt_src} â†’ {src_vocab_size} "
                    f"(å¢é‡è®­ç»ƒï¼Œ+{src_vocab_size - ckpt_src} ä¸ªæ–°è¯æ±‡)"
                )
            else:
                return False, (
                    f"âŒ æºè¯è¡¨ä¸å…¼å®¹:\n"
                    f"   æ£€æŸ¥ç‚¹: {ckpt_src}, å½“å‰: {src_vocab_size}"
                )
        else:
            messages.append(f"âœ… æºè¯è¡¨å…¼å®¹: {src_vocab_size}")
        
        # ç›®æ ‡è¯è¡¨æ£€æŸ¥
        if ckpt_tgt != tgt_vocab_size:
            if allow_size_increase and tgt_vocab_size > ckpt_tgt:
                messages.append(
                    f"â„¹ï¸  ç›®æ ‡è¯è¡¨æ‰©å±•: {ckpt_tgt} â†’ {tgt_vocab_size} "
                    f"(å¢é‡è®­ç»ƒï¼Œ+{tgt_vocab_size - ckpt_tgt} ä¸ªæ–°å­—ç¬¦)"
                )
            else:
                return False, (
                    f"âŒ ç›®æ ‡è¯è¡¨ä¸å…¼å®¹:\n"
                    f"   æ£€æŸ¥ç‚¹: {ckpt_tgt}, å½“å‰: {tgt_vocab_size}"
                )
        else:
            messages.append(f"âœ… ç›®æ ‡è¯è¡¨å…¼å®¹: {tgt_vocab_size}")
        
        return True, "\n".join(messages)
    
    def save_training_config(self, config: Dict[str, Any]):
        """ä¿å­˜è®­ç»ƒé…ç½®ï¼ˆç”¨äºå¤ç°ï¼‰ã€‚"""
        config_path = self.log_dir / 'config.json'
        with open(config_path, 'wb') as f:
            f.write(orjson.dumps(config, option=orjson.OPT_INDENT_2))
        self.logger.info(f"ğŸ“ è®­ç»ƒé…ç½®å·²ä¿å­˜: {config_path.name}")
    
    def update_training_history(
        self,
        epoch: int,
        loss: float,
        metrics: Dict[str, float] = None,
    ):
        """æ›´æ–°è®­ç»ƒå†å²è®°å½•ã€‚"""
        record = {
            'epoch': epoch,
            'loss': float(loss),
            'timestamp': datetime.now().isoformat(),
        }
        if metrics:
            record.update(metrics)
        
        self.training_history.append(record)
    
    def save_training_summary(self):
        """ä¿å­˜è®­ç»ƒæ‘˜è¦å’ŒæŒ‡æ ‡ã€‚"""
        summary = {
            'best_loss': self.best_loss,
            'best_epoch': self.best_epoch,
            'total_epochs': len(self.training_history),
            'training_duration': None,  # ç”±è°ƒç”¨è€…è®¾ç½®
            'history': self.training_history,
        }
        
        summary_path = self.log_dir / 'training_summary.json'
        with open(summary_path, 'wb') as f:
            f.write(orjson.dumps(summary, option=orjson.OPT_INDENT_2))
        
        self.logger.info(f"ğŸ“Š è®­ç»ƒæ‘˜è¦å·²ä¿å­˜: {summary_path.name}")
    
    def get_status_summary(self) -> str:
        """è·å–å½“å‰è®­ç»ƒçŠ¶æ€æ‘˜è¦ã€‚"""
        return (
            f"æœ€ä¼˜æ¨¡å‹: Epoch {self.best_epoch}, Loss={self.best_loss:.4f}\n"
            f"æ£€æŸ¥ç‚¹æ•°: {len(list(self.save_dir.glob('checkpoint_epoch*.pt')))}\n"
            f"è®­ç»ƒå†å²: {len(self.training_history)} è½®"
        )


# ä¾¿åˆ©å‡½æ•°ï¼šå¿«é€Ÿæ¢å¤æˆ–åˆå§‹åŒ–

def resume_or_init(
    save_dir: Path,
    model,
    optimizer,
    src_vocab,
    tgt_vocab,
    device='cpu',
    allow_vocab_increase=False,
) -> Tuple[int, TrainingCheckpointManager]:
    """
    ä¸€ä½“åŒ–çš„æ¢å¤æˆ–åˆå§‹åŒ–å‡½æ•°ã€‚
    
    Returns:
        (ä¸‹ä¸€ä¸ªepoch, æ£€æŸ¥ç‚¹ç®¡ç†å™¨)
    """
    logger = logging.getLogger(__name__)
    ckpt_mgr = TrainingCheckpointManager(save_dir)
    
    resumed, next_epoch, ckpt_data = ckpt_mgr.try_resume_from_checkpoint(
        model, optimizer, device
    )
    
    if resumed and ckpt_data:
        # éªŒè¯è¯è¡¨å…¼å®¹æ€§
        compatible, msg = ckpt_mgr.validate_vocab_compatibility(
            ckpt_data,
            len(src_vocab),
            len(tgt_vocab),
            allow_size_increase=allow_vocab_increase,
        )
        logger.info(msg)
        
        if not compatible:
            logger.error("âŒ è¯è¡¨ä¸å…¼å®¹ï¼Œæ— æ³•æ¢å¤")
            logger.info("ğŸ’¡ æç¤º: å¦‚æœè¿›è¡Œå¢é‡è®­ç»ƒï¼Œè¯·æ·»åŠ  --allow-vocab-increase å‚æ•°")
            raise RuntimeError("è¯è¡¨ä¸å…¼å®¹")
    
    return next_epoch, ckpt_mgr


# è¾…åŠ©å‡½æ•°ï¼šåŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹

def load_trained_model(
    model_path: Path,
    model,
    device='cpu',
    load_optimizer=False,
    optimizer=None,
) -> Dict[str, Any]:
    """
    åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹ï¼ˆbest_model.pt æˆ– checkpointï¼‰ã€‚
    
    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        model: æ¨¡å‹å®ä¾‹
        device: è®¾å¤‡
        load_optimizer: æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer: ä¼˜åŒ–å™¨å®ä¾‹ï¼ˆå¦‚æœ load_optimizer=Trueï¼‰
    
    Returns:
        åŠ è½½çš„æ£€æŸ¥ç‚¹ä¿¡æ¯
    """
    logger = logging.getLogger(__name__)
    
    if not model_path.exists():
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    ckpt = torch.load(str(model_path), map_location=device)
    model.load_state_dict(ckpt['model_state_dict'])
    
    if load_optimizer and optimizer and 'optimizer_state_dict' in ckpt:
        optimizer.load_state_dict(ckpt['optimizer_state_dict'])
    
    epoch = ckpt.get('epoch', '?')
    loss = ckpt.get('loss', '?')
    is_best = ckpt.get('best_flag', False)
    
    best_text = "ğŸŒŸ (æœ€ä¼˜æ¨¡å‹)" if is_best else ""
    logger.info(
        f"âœ… åŠ è½½æ¨¡å‹: {model_path.name}\n"
        f"   Epoch {epoch}, Loss={loss} {best_text}"
    )
    
    return ckpt
