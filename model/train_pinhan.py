"""è®­ç»ƒè„šæœ¬ï¼šæ‹¼éŸ³åˆ°æ±‰å­—çš„ Transformer æ¨¡å‹è®­ç»ƒ."""
import random
import sys
import logging
import time
from pathlib import Path
from datetime import datetime
import argparse
import orjson
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.optim as optim

sys.path.insert(0, str(Path(__file__).parent.parent / 'preprocess'))
from seq2seq_transformer import Vocab, Seq2SeqTransformer, generate_square_subsequent_mask
from pinyin_utils import normalize_pinyin_sequence, validate_pinyin_sequence

DATA_PATH = Path('data/clean_wiki.jsonl')
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


class PinyinHanziDataset(Dataset):
    """æ‹¼éŸ³->æ±‰å­—æ•°æ®é›†."""
    def __init__(
        self,
        path: Path,
        src_vocab: Vocab,
        tgt_vocab: Vocab,
        max_src_len: int = 64,
        max_tgt_len: int = 64,
        normalize_pinyin: bool = False,
        skip_invalid: bool = True,
    ) -> None:
        self.samples = []
        self.src_vocab = src_vocab
        self.tgt_vocab = tgt_vocab
        self.max_src_len = max_src_len
        self.max_tgt_len = max_tgt_len
        self.normalize_pinyin = normalize_pinyin
        self.skip_invalid = skip_invalid
        self.stats = {
            'total_lines': 0,
            'valid_samples': 0,
            'invalid_samples': 0,
            'mismatched_length': 0,
            'invalid_pinyin': 0,
            'empty_data': 0,
        }
        self._load_data(path)

    def _load_data(self, path: Path) -> None:
        """åŠ è½½å’Œå¤„ç†æ•°æ®."""
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                self.stats['total_lines'] += 1
                line = line.strip()
                if not line:
                    self.stats['empty_data'] += 1
                    continue
                try:
                    j = orjson.loads(line)
                except Exception:
                    self.stats['invalid_samples'] += 1
                    continue
                hanzi = j.get('hanzi', '').strip()
                pinyin = j.get('pinyin', '').strip()
                if not hanzi or not pinyin:
                    self.stats['empty_data'] += 1
                    continue
                if self.normalize_pinyin:
                    try:
                        pinyin = normalize_pinyin_sequence(pinyin)
                    except Exception:
                        self.stats['invalid_pinyin'] += 1
                        if self.skip_invalid:
                            continue
                if self.skip_invalid and not validate_pinyin_sequence(pinyin):
                    self.stats['invalid_pinyin'] += 1
                    continue
                src_tokens = pinyin.split()
                tgt_tokens = list(hanzi)
                if len(src_tokens) == 0 or len(tgt_tokens) == 0:
                    self.stats['empty_data'] += 1
                    continue
                if len(src_tokens) != len(tgt_tokens):
                    self.stats['mismatched_length'] += 1
                    if self.skip_invalid:
                        continue
                if len(src_tokens) > (self.max_src_len - 2):
                    src_tokens = src_tokens[:self.max_src_len - 2]
                    tgt_tokens = tgt_tokens[:self.max_src_len - 2]
                if len(tgt_tokens) > (self.max_tgt_len - 2):
                    tgt_tokens = tgt_tokens[:self.max_tgt_len - 2]
                self.samples.append((src_tokens, tgt_tokens))
                self.stats['valid_samples'] += 1

    def __len__(self) -> int:
        return len(self.samples)

    def __getitem__(self, idx: int) -> tuple:
        return self.samples[idx]

    def print_statistics(self) -> None:
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯."""
        logger = logging.getLogger()
        logger.info("\n=== æ•°æ®é›†ç»Ÿè®¡ ===")
        for key, val in self.stats.items():
            logger.info(f"{key}: {val}")
        if self.stats['total_lines'] > 0:
            valid_ratio = self.stats['valid_samples'] / self.stats['total_lines'] * 100
            logger.info(f"æœ‰æ•ˆæ ·æœ¬æ¯”ä¾‹: {valid_ratio:.2f}%")


def collate_fn(batch: list, src_vocab: Vocab, tgt_vocab: Vocab) -> tuple:
    """æ‰¹å¤„ç†å‡½æ•°."""
    srcs, tgts = zip(*batch)
    src_ids = [src_vocab.encode(list(s), add_bos_eos=True) for s in srcs]
    tgt_ids = [tgt_vocab.encode(list(t), add_bos_eos=True) for t in tgts]
    max_src = max(len(x) for x in src_ids)
    max_tgt = max(len(x) for x in tgt_ids)
    pad_id_src = src_vocab.token_to_id[src_vocab.pad_token]
    pad_id_tgt = tgt_vocab.token_to_id[tgt_vocab.pad_token]
    src_padded = [x + [pad_id_src] * (max_src - len(x)) for x in src_ids]
    tgt_padded = [x + [pad_id_tgt] * (max_tgt - len(x)) for x in tgt_ids]
    src_tensor = torch.LongTensor(src_padded).transpose(0, 1)
    tgt_tensor = torch.LongTensor(tgt_padded).transpose(0, 1)
    return src_tensor, tgt_tensor


def build_vocabs(path: Path, max_pinyin_tokens: int = 50000, max_hanzi_chars: int = 50000) -> tuple:
    """æ„å»ºè¯è¡¨."""
    logger = logging.getLogger()
    pinyin_counter = {}
    hanzi_counter = {}
    polyphonic_stats = {}
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                j = orjson.loads(line)
            except Exception:
                continue
            p = j.get('pinyin', '')
            h = j.get('hanzi', '')
            if not p or not h:
                continue
            pinyins = p.strip().split()
            hanzis = list(h.strip())
            for pinyin_token, hanzi_char in zip(pinyins, hanzis):
                pinyin_counter[pinyin_token] = pinyin_counter.get(pinyin_token, 0) + 1
                hanzi_counter[hanzi_char] = hanzi_counter.get(hanzi_char, 0) + 1
                if hanzi_char not in polyphonic_stats:
                    polyphonic_stats[hanzi_char] = set()
                polyphonic_stats[hanzi_char].add(pinyin_token)
    polyphonic_hanzis = {h: p for h, p in polyphonic_stats.items() if len(p) > 1}
    pinyin_sorted = sorted(pinyin_counter.items(), key=lambda x: -x[1])
    hanzi_sorted = sorted(hanzi_counter.items(), key=lambda x: -x[1])
    pinyin_tokens = [t for t, _ in pinyin_sorted][:max_pinyin_tokens]
    hanzi_tokens = [t for t, _ in hanzi_sorted][:max_hanzi_chars]
    src_vocab = Vocab(pinyin_tokens)
    tgt_vocab = Vocab(hanzi_tokens)
    logger.info(f"æ‹¼éŸ³ tokens: {len(pinyin_tokens)}")
    logger.info(f"æ±‰å­—æ€»æ•°: {len(hanzi_tokens)}")
    logger.info(f"å¤šéŸ³å­—æ•°é‡: {len(polyphonic_hanzis)}")
    return src_vocab, tgt_vocab


def train_one_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    optimizer: optim.Optimizer,
    criterion: nn.Module,
    src_vocab: Vocab,
    tgt_vocab: Vocab,
    device: torch.device,
    epoch: int,
) -> tuple[float, float]:
    """è®­ç»ƒä¸€ä¸ª epoch, è¿”å› (avg_loss, avg_grad_norm)."""
    logger = logging.getLogger()
    model.train()
    total_loss = 0.0
    total_grad_norm = 0.0
    epoch_start = time.time()
    num_batches = len(dataloader)
    
    for i, (src, tgt) in enumerate(dataloader):
        batch_start = time.time()
        src = src.to(device)
        tgt = tgt.to(device)
        tgt_input = tgt[:-1, :]
        tgt_out = tgt[1:, :]
        tgt_mask = generate_square_subsequent_mask(tgt_input.size(0)).to(device)
        src_key_padding_mask = (src.transpose(0, 1) == src_vocab.token_to_id[src_vocab.pad_token])
        tgt_key_padding_mask = (tgt_input.transpose(0, 1) == tgt_vocab.token_to_id[tgt_vocab.pad_token])
        
        # ğŸ”§ è½¬æ¢maskç±»å‹ä¸ºfloat, ä¿è¯ä¸€è‡´æ€§
        tgt_mask = tgt_mask.float()
        src_key_padding_mask = src_key_padding_mask.float()
        tgt_key_padding_mask = tgt_key_padding_mask.float()
        optimizer.zero_grad()
        logits = model(
            src,
            tgt_input,
            tgt_mask=tgt_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=src_key_padding_mask,
        )
        loss = criterion(logits.view(-1, logits.size(-1)), tgt_out.reshape(-1))
        loss.backward()
        
        # ç›‘æ§æ¢¯åº¦
        grad_norm = log_gradient_stats(model, logger)
        total_grad_norm += grad_norm
        
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        total_loss += loss.item()
        
        # å®šæœŸè¾“å‡ºè¿›åº¦
        if i % max(1, num_batches // 10) == 0:
            batch_time = time.time() - batch_start
            progress = (i + 1) / num_batches
            elapsed = time.time() - epoch_start
            eta = elapsed / (i + 1) * (num_batches - i - 1) if i > 0 else 0
            
            logger.info(
                f"Epoch {epoch} [{i+1:>4d}/{num_batches}] "
                f"loss={loss.item():.4f} "
                f"grad_norm={grad_norm:.4f} "
                f"time={batch_time:.2f}s "
                f"ETA={eta:.0f}s"
            )
    
    epoch_time = time.time() - epoch_start
    avg_loss = total_loss / len(dataloader)
    avg_grad_norm = total_grad_norm / len(dataloader)
    
    return avg_loss, avg_grad_norm, epoch_time


def save_checkpoint(
    model: nn.Module,
    optimizer: optim.Optimizer,
    epoch: int,
    out_dir: Path,
    src_vocab_size: int = None,
    tgt_vocab_size: int = None,
) -> None:
    """ä¿å­˜ checkpoint (å«è¯è¡¨å¤§å°ç”¨äºéªŒè¯)."""
    out_dir.mkdir(parents=True, exist_ok=True)
    ckpt = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'src_vocab_size': src_vocab_size,
        'tgt_vocab_size': tgt_vocab_size,
    }
    torch.save(ckpt, out_dir / f'checkpoint_epoch{epoch}.pt')


def log_gradient_stats(model: nn.Module, logger: logging.Logger) -> float:
    """è®¡ç®—å¹¶è®°å½•æ¢¯åº¦ç»Ÿè®¡."""
    total_norm = 0.0
    for param in model.parameters():
        if param.grad is not None:
            total_norm += param.grad.norm().item() ** 2
    total_norm = (total_norm) ** 0.5
    
    if total_norm > 100:
        logger.warning(f"âš ï¸  æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {total_norm:.4f} (å¯èƒ½çˆ†ç‚¸)")
    elif total_norm < 1e-8:
        logger.warning(f"âš ï¸  æ¢¯åº¦èŒƒæ•°è¿‡å°: {total_norm:.4e} (å¯èƒ½æ¶ˆå¤±)")
    
    return total_norm


def get_memory_usage() -> float:
    """è·å–å½“å‰GPU/CPUå†…å­˜ä½¿ç”¨ (GB)."""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024 ** 3
    return 0.0


def log_training_start(
    logger: logging.Logger,
    model: nn.Module,
    data_size: int,
    batch_size: int,
    epochs: int,
    lr: float,
    device: torch.device,
) -> None:
    """è®°å½•è®­ç»ƒå¼€å§‹æ—¶çš„è¯¦ç»†ä¿¡æ¯."""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info("="*70)
    logger.info("ğŸš€ è®­ç»ƒé…ç½®")
    logger.info("="*70)
    logger.info(f"è®¾å¤‡: {device}")
    logger.info(f"æ¨¡å‹å‚æ•°: {total_params:,} (å¯è®­ç»ƒ: {trainable_params:,})")
    logger.info(f"æ•°æ®é›†å¤§å°: {data_size:,}")
    logger.info(f"æ‰¹å¤§å°: {batch_size}, æ€»æ‰¹æ•°: {(data_size + batch_size - 1) // batch_size}")
    logger.info(f"è½®æ•°: {epochs}, å­¦ä¹ ç‡: {lr:.6f}")
    logger.info(f"åˆå§‹å†…å­˜: {get_memory_usage():.2f} GB")
    logger.info("="*70)


def setup_logging(log_file: str | None = None) -> logging.Logger:
    """é…ç½®æ—¥å¿—."""
    fmt = '%(asctime)s %(levelname)s: %(message)s'
    logging.basicConfig(level=logging.INFO, format=fmt, force=True)
    logger = logging.getLogger()
    if log_file:
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)
        fh = logging.FileHandler(log_file, encoding='utf-8')
        fh.setLevel(logging.INFO)
        fh.setFormatter(logging.Formatter(fmt))
        logger.addHandler(fh)
    return logger


def main() -> None:
    """ä¸»è®­ç»ƒå‡½æ•°."""
    parser = argparse.ArgumentParser(description='è®­ç»ƒæ‹¼éŸ³->æ±‰å­— Transformer')
    parser.add_argument('--data', type=str, default=str(DATA_PATH), help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=3, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=32, help='æ‰¹å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--save-dir', type=str, default='outputs/pinhan_model', help='ä¿å­˜ç›®å½•')
    parser.add_argument('--max-samples', type=int, default=0, help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆ0 è¡¨ç¤ºå…¨éƒ¨ï¼‰')
    parser.add_argument('--resume', action='store_true', help='ä» checkpoint æ¢å¤')
    parser.add_argument('--log-file', type=str, default=None, help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--normalize-pinyin', action='store_true', help='è§„èŒƒåŒ–æ‹¼éŸ³')
    args = parser.parse_args()
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    default_log = f'outputs/train_{timestamp}.log'
    log_file = args.log_file or default_log
    logger = setup_logging(log_file)
    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info(f"æ•°æ®è·¯å¾„: {args.data}")
    logger.info(f"æ‰¹å¤§å°: {args.batch_size}, å­¦ä¹ ç‡: {args.lr}, è½®æ•°: {args.epochs}")
    logger.info("æ„å»ºè¯è¡¨...")
    src_vocab, tgt_vocab = build_vocabs(Path(args.data))
    logger.info(f"æºè¯è¡¨: {len(src_vocab)}, ç›®æ ‡è¯è¡¨: {len(tgt_vocab)}")
    logger.info("åŠ è½½æ•°æ®é›†...")
    ds = PinyinHanziDataset(
        Path(args.data),
        src_vocab,
        tgt_vocab,
        max_src_len=64,
        max_tgt_len=64,
        normalize_pinyin=args.normalize_pinyin,
    )
    ds.print_statistics()
    assert src_vocab.pad_token == tgt_vocab.pad_token, "src å’Œ tgt çš„ pad_token å¿…é¡»ç›¸åŒ"
    if args.max_samples > 0 and len(ds) > args.max_samples:
        indices = random.sample(range(len(ds)), args.max_samples)
        ds.samples = [ds.samples[i] for i in indices]
        logger.info(f"ä½¿ç”¨ {len(ds)} ä¸ªæ ·æœ¬")
    dataloader = DataLoader(
        ds,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab),
    )
    save_dir = Path(args.save_dir)
    model = Seq2SeqTransformer(
        len(src_vocab),
        len(tgt_vocab),
        d_model=256,
        nhead=4,
        num_encoder_layers=3,
        num_decoder_layers=3,
        pad_idx_src=src_vocab.token_to_id[src_vocab.pad_token],
        pad_idx_tgt=tgt_vocab.token_to_id[tgt_vocab.pad_token],
    )
    model = model.to(DEVICE)
    
    # ğŸ”´ æ–°å¢: è®°å½•è®­ç»ƒå¼€å§‹è¯¦æƒ…
    log_training_start(
        logger,
        model,
        len(ds),
        args.batch_size,
        args.epochs,
        args.lr,
        DEVICE,
    )
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.token_to_id[tgt_vocab.pad_token])
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=2
    )
    start_epoch = 1
    if args.resume:
        ckpts = sorted(save_dir.glob('checkpoint_epoch*.pt'))
        if ckpts:
            latest = ckpts[-1]
            logger.info(f"ä» {latest} æ¢å¤")
            ck = torch.load(str(latest), map_location=DEVICE)
            
            # ğŸ”´ æ–°å¢: è¯è¡¨ä¸€è‡´æ€§æ£€æŸ¥
            old_src_size = ck.get('src_vocab_size', None)
            old_tgt_size = ck.get('tgt_vocab_size', None)
            
            if old_src_size is not None and old_src_size != len(src_vocab):
                logger.error(
                    f"âŒ æºè¯è¡¨å¤§å°ä¸åŒ¹é…: "
                    f"checkpoint={old_src_size}, å½“å‰={len(src_vocab)}"
                )
                logger.warning("æç¤º: å¦‚æœæ˜¯å¢é‡è®­ç»ƒä¸”æœ‰æ–°è¯è¯­æ˜¯æ­£å¸¸çš„")
                logger.warning("å¦‚éœ€å¼ºåˆ¶ç»§ç»­ï¼Œè¯·é‡æ–°è¿è¡Œä¸åŠ --resumeå‚æ•°")
                raise RuntimeError("è¯è¡¨ä¸å…¼å®¹")
            
            if old_tgt_size is not None and old_tgt_size != len(tgt_vocab):
                logger.error(
                    f"âŒ ç›®æ ‡è¯è¡¨å¤§å°ä¸åŒ¹é…: "
                    f"checkpoint={old_tgt_size}, å½“å‰={len(tgt_vocab)}"
                )
                raise RuntimeError("è¯è¡¨ä¸å…¼å®¹")
            
            model.load_state_dict(ck['model_state_dict'])
            optimizer.load_state_dict(ck['optimizer_state_dict'])
            start_epoch = ck.get('epoch', 1) + 1
            logger.info(f"âœ… ä»epoch {ck.get('epoch', '?')} æ¢å¤ï¼Œä¸‹ä¸€è½®ä» {start_epoch} å¼€å§‹")
    logger.info(f"å¼€å§‹è®­ç»ƒ {args.epochs} è½®...")
    train_start_time = time.time()
    
    for ep in range(start_epoch, args.epochs + 1):
        avg_loss, avg_grad_norm, epoch_time = train_one_epoch(
            model, dataloader, optimizer, criterion, src_vocab, tgt_vocab, DEVICE, ep
        )
        
        # è®°å½•å½“å‰å­¦ä¹ ç‡
        current_lr = optimizer.param_groups[0]['lr']
        
        # è°ƒæ•´å­¦ä¹ ç‡
        old_lr = current_lr
        scheduler.step(avg_loss)
        new_lr = optimizer.param_groups[0]['lr']
        lr_change = " (â†“ LR)" if new_lr < old_lr else ""
        
        # ä¿å­˜checkpoint
        save_checkpoint(model, optimizer, ep, save_dir, len(src_vocab), len(tgt_vocab))
        src_vocab.save(str(save_dir / 'src_vocab.json'))
        tgt_vocab.save(str(save_dir / 'tgt_vocab.json'))
        
        # è®¡ç®—æ€»è¿›åº¦
        total_time = time.time() - train_start_time
        avg_time_per_epoch = total_time / (ep - start_epoch + 1)
        remaining_epochs = args.epochs - ep
        eta_seconds = avg_time_per_epoch * remaining_epochs
        
        logger.info("-" * 70)
        logger.info(
            f"Epoch {ep:3d}/{args.epochs} | "
            f"Loss: {avg_loss:.4f} | "
            f"Grad: {avg_grad_norm:.4f} | "
            f"LR: {new_lr:.6f}{lr_change} | "
            f"Time: {epoch_time:.1f}s | "
            f"ETA: {eta_seconds:.0f}s | "
            f"Mem: {get_memory_usage():.2f}GB"
        )
        logger.info("-" * 70)
    
    total_train_time = time.time() - train_start_time
    logger.info(f"\nâœ… è®­ç»ƒå®Œæˆ! æ€»è€—æ—¶: {total_train_time:.1f}s")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹åŠå®Œæ•´å…ƒæ•°æ®
    final_model_data = {
        'model_state_dict': model.state_dict(),
        'config': {
            'd_model': 256,
            'nhead': 4,
            'num_encoder_layers': 3,
            'num_decoder_layers': 3,
            'src_vocab_size': len(src_vocab),
            'tgt_vocab_size': len(tgt_vocab),
        },
        'metadata': {
            'epoch': args.epochs,
            'loss': avg_loss,
            'timestamp': datetime.now().isoformat(),
            'device': str(DEVICE),
            'total_time': total_train_time,
            'data_source': args.data,
        },
        'vocab_info': {
            'src_vocab_size': len(src_vocab),
            'tgt_vocab_size': len(tgt_vocab),
        }
    }
    torch.save(final_model_data, save_dir / 'model.pt')
    logger.info(f"ğŸ“¦ æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}/model.pt (å«å…ƒæ•°æ®)")


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        logging.exception("è®­ç»ƒå‡ºé”™")
        raise
