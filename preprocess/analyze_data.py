#!/usr/bin/env python3
"""
preprocess/analyze_data.py

åˆ†æ JSONL æ•°æ®æ–‡ä»¶çš„ç»Ÿè®¡ç‰¹å¾ï¼ˆå£°è°ƒåˆ†å¸ƒã€æ±‰å­—é¢‘ç‡ã€åºåˆ—é•¿åº¦ç­‰ï¼‰ã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
    python preprocess/analyze_data.py data/wiki_7k.jsonl
    python preprocess/analyze_data.py data/wiki_7k.jsonl --top 20
"""
import json
import sys
from pathlib import Path
from collections import Counter
from typing import Dict, List, Tuple
import argparse


def analyze_dataset(filepath: str, top_n: int = 10, show_samples: int = 5) -> Dict:
    """
    åˆ†æ JSONL æ•°æ®é›†çš„ç»Ÿè®¡ç‰¹å¾ã€‚
    
    Args:
        filepath: æ•°æ®æ–‡ä»¶è·¯å¾„
        top_n: æ˜¾ç¤ºå‰ N ä¸ªé¢‘ç¹å…ƒç´ 
        show_samples: æ˜¾ç¤ºæ ·æœ¬æ•°é‡
    
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    if not Path(filepath).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
        sys.exit(1)
    
    # ç»Ÿè®¡å˜é‡
    hanzi_freq = Counter()
    pinyin_freq = Counter()
    tone_stats = Counter()
    length_stats = []
    samples = []
    total_lines = 0
    valid_lines = 0
    punctuation_count = 0
    no_tone_count = 0
    mismatch_count = 0
    
    # æ ‡ç‚¹ç¬¦å·é›†åˆ
    punctuation = 'ã€‚ï¼ï¼Ÿï¼›ï¼Œï¼š""''Â·â€¦Â·ï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€ï½ã€ï¼›'
    
    print(f"ğŸ“‚ åˆ†ææ–‡ä»¶: {filepath}\n")
    print("æ­£åœ¨æ‰«ææ•°æ®...", end='', flush=True)
    
    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
        for line_no, line in enumerate(f, 1):
            total_lines += 1
            
            try:
                obj = json.loads(line.strip())
                hanzi = obj.get('hanzi', '').strip()
                pinyin = obj.get('pinyin', '').strip()
                
                if not hanzi or not pinyin:
                    continue
                
                valid_lines += 1
                
                # å­˜å‚¨å‰å‡ ä¸ªæ ·æœ¬
                if len(samples) < show_samples:
                    samples.append({'hanzi': hanzi, 'pinyin': pinyin})
                
                # ç»Ÿè®¡æ±‰å­—
                for char in hanzi:
                    hanzi_freq[char] += 1
                
                # ç»Ÿè®¡æ‹¼éŸ³
                py_tokens = pinyin.split()
                pinyin_freq.update(py_tokens)
                length_stats.append(len(py_tokens))
                
                # æ£€æŸ¥é•¿åº¦åŒ¹é…
                if len(hanzi) != len(py_tokens):
                    mismatch_count += 1
                
                # ç»Ÿè®¡å£°è°ƒ
                tone_found = False
                for py_token in py_tokens:
                    if py_token and py_token[-1].isdigit():
                        tone = py_token[-1]
                        tone_stats[f"å£°è°ƒ{tone}"] += 1
                        tone_found = True
                    else:
                        no_tone_count += 1
                
                # æ£€æµ‹æ ‡ç‚¹
                if any(p in hanzi for p in punctuation):
                    punctuation_count += 1
                    
            except Exception as e:
                continue
            
            if line_no % 1000 == 0:
                print(f"\ræ­£åœ¨æ‰«ææ•°æ®... {line_no:,} è¡Œ", end='', flush=True)
    
    print(f"\râœ… æ‰«æå®Œæˆ                     \n")
    
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    total_tokens = sum(length_stats) if length_stats else 0
    avg_length = sum(length_stats) / len(length_stats) if length_stats else 0
    
    # è¾“å‡ºç»“æœ
    print("=" * 80)
    print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 80)
    print(f"æ€»è¡Œæ•°: {total_lines:,}")
    print(f"æœ‰æ•ˆæ ·æœ¬: {valid_lines:,} ({100*valid_lines/total_lines:.1f}%)")
    print(f"æ€» token æ•°: {total_tokens:,}")
    print(f"ä¸åŒæ±‰å­—æ•°: {len(hanzi_freq):,}")
    print(f"ä¸åŒæ‹¼éŸ³æ•°: {len(pinyin_freq):,}")
    
    print(f"\nğŸ“ åºåˆ—é•¿åº¦ç»Ÿè®¡:")
    print(f"  å¹³å‡é•¿åº¦: {avg_length:.2f}")
    print(f"  æœ€çŸ­: {min(length_stats)} æœ€é•¿: {max(length_stats)}")
    print(f"  é•¿åº¦åŒ¹é…é”™è¯¯: {mismatch_count:,} ({100*mismatch_count/valid_lines:.1f}%)")
    
    print(f"\nğŸ“ å£°è°ƒåˆ†å¸ƒ:")
    total_tones = sum(tone_stats.values())
    for tone in ['å£°è°ƒ1', 'å£°è°ƒ2', 'å£°è°ƒ3', 'å£°è°ƒ4']:
        count = tone_stats.get(tone, 0)
        pct = 100 * count / total_tones if total_tones > 0 else 0
        bar_length = int(pct / 2)
        bar = 'â–ˆ' * bar_length + 'â–‘' * (50 - bar_length)
        print(f"  {tone}: {count:6,} ({pct:5.1f}%) {bar}")
    print(f"  æ— å£°è°ƒ: {no_tone_count:6,} ({100*no_tone_count/total_tokens:.1f}% çš„ token)")
    
    print(f"\nâš ï¸  åŒ…å«æ ‡ç‚¹çš„æ ·æœ¬: {punctuation_count:,} ({100*punctuation_count/valid_lines:.1f}%)")
    
    print(f"\nğŸ”¤ Top {top_n} å¸¸è§æ±‰å­—:")
    for i, (char, count) in enumerate(hanzi_freq.most_common(top_n), 1):
        print(f"  {i:2d}. '{char}': {count:6,}")
    
    print(f"\nğŸ”¤ Top {top_n} å¸¸è§æ‹¼éŸ³:")
    for i, (py, count) in enumerate(pinyin_freq.most_common(top_n), 1):
        print(f"  {i:2d}. {py:10s}: {count:6,}")
    
    print(f"\nğŸ“‹ æ ·æœ¬ç¤ºä¾‹ (å‰ {len(samples)} æ¡):")
    print("-" * 80)
    for i, sample in enumerate(samples, 1):
        print(f"{i}. hanzi: '{sample['hanzi']}'")
        print(f"   pinyin: '{sample['pinyin']}'")
        print()
    
    return {
        'total_lines': total_lines,
        'valid_lines': valid_lines,
        'total_tokens': total_tokens,
        'unique_hanzi': len(hanzi_freq),
        'unique_pinyin': len(pinyin_freq),
        'avg_length': avg_length,
        'tone_stats': dict(tone_stats),
    }


def main():
    parser = argparse.ArgumentParser(
        description='åˆ†æ JSONL æ•°æ®æ–‡ä»¶çš„ç»Ÿè®¡ç‰¹å¾',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ï¼š
  åˆ†ææ•°æ®æ–‡ä»¶:
    python preprocess/analyze_data.py data/wiki_7k.jsonl
  
  æ˜¾ç¤ºå‰ 20 ä¸ªé¢‘ç¹å…ƒç´ :
    python preprocess/analyze_data.py data/wiki_7k.jsonl --top 20
  
  æ˜¾ç¤ºå‰ 10 ä¸ªæ ·æœ¬:
    python preprocess/analyze_data.py data/wiki_7k.jsonl --samples 10
        '''
    )
    
    parser.add_argument('input', help='è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--top', type=int, default=10, help='æ˜¾ç¤ºå‰ N ä¸ªé¢‘ç¹å…ƒç´ ï¼ˆé»˜è®¤ 10ï¼‰')
    parser.add_argument('--samples', type=int, default=5, help='æ˜¾ç¤ºæ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 5ï¼‰')
    
    args = parser.parse_args()
    analyze_dataset(args.input, top_n=args.top, show_samples=args.samples)


if __name__ == '__main__':
    main()
